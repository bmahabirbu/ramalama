#!/usr/bin/env python3
import json
import argparse
import hashlib
import os
import sys
import uuid
from pathlib import Path
import time
import threading
import itertools

import docling
from docling.chunking import HybridChunker
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling_core.types.doc import DoclingDocument

# Docling Setup (Turn off OCR (image processing) for drastically reduced RAM usage and big speed increase)
pipeline_options = PdfPipelineOptions()
pipeline_options.do_ocr = False
doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )

# Global Vars
EMBED_MODEL = os.getenv("EMBED_MODEL", "jinaai/jina-embeddings-v2-small-en")
SPARSE_MODEL = os.getenv("SPARSE_MODEL", "prithivida/Splade_PP_en_v1")
COLLECTION_NAME = "rag"

class Converter:
    """A Class designed to handle all document conversions using Docling"""

    def __init__(self, output, targets):
        self.doc_converter = doc_converter
        self.targets = []
        for target in targets:
            self.add(target)
        self.output = output

    def add(self, file_path):
        if os.path.isdir(file_path):
            self.walk(file_path)  # Walk directory and process all files
        else:
            self.targets.append(file_path)  # Process the single file
    
    def show_progress(self, message, stop_event):
            spinner = itertools.cycle([".", "..", "..."])
            while not stop_event.is_set():
                sys.stdout.write(f"\r{message} {next(spinner)}   ")
                sys.stdout.flush()
                time.sleep(0.5)
            sys.stdout.write("\r" + " " * 50 + "\r")

    def convert(self, choice):
        results = []
        names = []

        for target in self.targets:
            name = self.get_name(str(target))
            names.append(name)
            stop_event = threading.Event()
            progress_thread = threading.Thread(target=self.show_progress, args=(f"Converting {name}.pdf to json", stop_event))
            progress_thread.start()
        
            try:
                results.append(self.doc_converter.convert(target))
            finally:
                stop_event.set()
                progress_thread.join()
            
            print(f"Finished converting {name}.pdf to {name}.json")

        if choice == "json":
            self.export_json(results, names)
        elif choice == "milvus":
            self.export_milvus(results)
        else:
            self.export_qdrant(results)
    
    def generate_hash(self, document: str) -> int:
        """Generate a unique int64 hash from the document text."""
        sha256_hash = hashlib.sha256(document.encode('utf-8')).hexdigest()
        uuid_val = uuid.UUID(sha256_hash[:32])
        # Convert to signed int64 (Milvus requires signed 64-bit)
        return uuid_val.int & ((1 << 63) - 1)
    
    def chunk(self, docs):
        chunker = HybridChunker(tokenizer=EMBED_MODEL, overlap=100, merge_peers=True)
        documents, ids = [], []
        
        for file in docs:
            chunk_iter = chunker.chunk(dl_doc=file.document)
            for chunk in chunk_iter:
                # Extract the enriched text from the chunk
                doc_text = chunker.contextualize(chunk=chunk)
                # Append to respective lists
                documents.append(doc_text)
                # Generate unique ID for the chunk
                doc_id = self.generate_hash(doc_text)
                ids.append(doc_id)
        return documents, ids
    
    def export_json(self, docs, names):
        print("Exporting to JSON")
        output_dir = Path(self.output)
        output_dir.mkdir(parents=True, exist_ok=True)

        for file, name in zip(docs, names):
            doc = file.document.export_to_dict()
            output_path = output_dir / f"{name}.json"
            with output_path.open("w") as fp:
                fp.write(json.dumps(doc))

    def export_qdrant(self, docs):
        print("Exporting to Qdrant")
        output_dir = Path(self.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        # optional import qdrant
        try:
            import qdrant_client
            from qdrant_client import models
        except ImportError:
            print('Qdrant Vectordb is disabled ... returning')
            return
        client = qdrant_client.QdrantClient(path=self.output)
        client.set_model(EMBED_MODEL)
        client.set_sparse_model(SPARSE_MODEL)
        chunks, ids = self.chunk(docs)
        client.add(COLLECTION_NAME, documents=chunks, ids=ids, batch_size=1)
        
    def export_milvus(self, docs):
        # TODO: Add hybird search support
        print("Exporting to Milvus")
        output_dir = Path(self.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        # optional import milvus
        try:
            from pymilvus import MilvusClient
            from fastembed.embedding import TextEmbedding
            print('Milvus enabled')
        except ImportError:
            print('Milvus Vectordb is disabled ... returning')
            return

        data = []

        qmodel = TextEmbedding(model_name=EMBED_MODEL)
        test_embedding = next(qmodel.embed("This is a test"))
        embedding_dim = len(test_embedding)

        milvus_client = MilvusClient(uri=os.path.join(self.output, "milvus.db"))
        collection_name = COLLECTION_NAME

        milvus_client.create_collection(
            collection_name=collection_name,
            dimension=embedding_dim,
        )
        chunks, ids = self.chunk(docs)
        batch_size = 5
        data = []
        for i, (chunk, id) in enumerate(zip(chunks, ids)):
            embedding = next(qmodel.embed([chunk]))
            milvus_client.insert(
                collection_name=collection_name,
                data=[{
                    "id": id,
                    "vector": embedding,
                    "text": chunk
                }]
            )
            # Flush every 100 records to reduce RAM usage
            if i % 100 == 0: 
                milvus_client.flush(collection_name=collection_name)
            
            print(f"\rProcessed {i+1}/{len(chunks)}", end='', flush=True) 

        milvus_client.flush(collection_name=collection_name)
                    
    def get_name(self, path):
        file_name = Path(path).stem
        return file_name

    def walk(self, path):
        for root, dirs, files in os.walk(path, topdown=True):
            if len(files) == 0:
                continue
            for f in files:
                file = os.path.join(root, f)
                if os.path.isfile(file):
                    self.targets.append(file)
def load():
    # Dummy code to preload models
    converter = DocumentConverter()
    converter.initialize_pipeline(InputFormat.PDF)

parser = argparse.ArgumentParser(
    prog="docling",
    description="process source files into RAG vector database",
)

parser.add_argument("target", nargs="?", help="Target database")
parser.add_argument("source", nargs="*", help="Source files")
parser.add_argument("--choice", choices=["json", "qdrant", "milvus"], default="qdrant", help="Conversion output type")

def perror(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

def eprint(e, exit_code):
    perror("Error: " + str(e).strip("'\""))
    sys.exit(exit_code)

try:
    args = parser.parse_args()
    if args.target == "load":
        load()
    else:
        converter = Converter(args.target, args.source)
        print(args.choice)
        converter.convert(args.choice)
except docling.exceptions.ConversionError as e:
    eprint(e, 1)
except FileNotFoundError as e:
    eprint(e, 1)
except ValueError as e:
    eprint(e, 1)
except KeyboardInterrupt:
    pass
