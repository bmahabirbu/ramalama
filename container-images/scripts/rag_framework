#!/usr/bin/env python3

# AI imports
import cmd
import qdrant_client
from qdrant_client import models
import openai
from fastembed.rerank.cross_encoder import TextCrossEncoder
from docling.document_converter import DocumentConverter
from docling.chunking import HybridChunker
from docling_core.types.doc import DoclingDocument
# Regular imports
import uuid
import os
import hashlib
import argparse
from pathlib import Path
import json
import sys

# Global Vars
EMBED_MODEL = os.getenv("EMBED_MODEL", "jinaai/jina-embeddings-v2-small-en")
SPARSE_MODEL = os.getenv("SPARSE_MODEL", "prithivida/Splade_PP_en_v1")
RANK_MODEL = os.getenv("RANK_MODEL", "Xenova/ms-marco-MiniLM-L-6-v2")
COLLECTION_NAME = "rag"
# Needed for mac to not give errors
os.environ["TOKENIZERS_PARALLELISM"] = "true"


def eprint(e, exit_code):
    print("Error:", str(e).strip("'\""), file=sys.stderr)
    sys.exit(exit_code)

# Helper Classes and Functions

class Rag(cmd.Cmd):
    prompt = "> "
    def __init__(self, vector_path):
        # Initlialze the cmd class
        super().__init__()

        # get MD
        self.vector_path = vector_path

        self.client = qdrant_client.QdrantClient(location=":memory:")
        self.client.set_model(EMBED_MODEL)
        self.client.set_sparse_model(SPARSE_MODEL)
        self.reranker = TextCrossEncoder(model_name=RANK_MODEL)
        # optimizations to reduce ram
        self.client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=self.client.get_fastembed_vector_params(on_disk=True),
            sparse_vectors_config= self.client.get_fastembed_sparse_vector_params(on_disk=True),
            quantization_config=models.ScalarQuantization(
                scalar=models.ScalarQuantizationConfig(
                    type=models.ScalarType.INT8,
                    always_ram=True,
                ),
            ),
        )

        # ingest md
        self.ingest()

        # Setup openai api
        self.llm = openai.OpenAI(api_key="your-api-key", base_url="http://localhost:8080")
        self.chat_history = []  # Store chat history

    def do_EOF(self, user_content):
        print("")
        return True
    
    def add(self, file_path):
        if os.path.isdir(file_path):
            self.walk(file_path)  # Walk directory and process all files
        else:
            self.targets.append(file_path)  # Process the single file
    
    def walk(self, path):
        for root, dirs, files in os.walk(path, topdown=True):
            if len(files) == 0:
                continue
            for f in files:
                file = os.path.join(root, f)
                if os.path.isfile(file):
                    self.targets.append(file)

    def ingest(self):
        json_dir = Path(self.vector_path)
        documents, metadata, ids = [], [], []
        chunker = HybridChunker(tokenizer=EMBED_MODEL, overlap=100, merge_peers=True)

        for json_file in json_dir.glob("*.json"):
            with json_file.open("r") as fp:
                doc_dict = json.load(fp)
                result = DoclingDocument.model_validate(doc_dict)

            chunk_iter = chunker.chunk(dl_doc=result)
            for chunk in chunk_iter:
                doc_text = chunker.contextualize(chunk=chunk)
                documents.append(doc_text)
                doc_id = self.generate_hash(doc_text)
                ids.append(doc_id)

        return self.client.add(COLLECTION_NAME, documents=documents, ids=ids, batch_size=1)
    
    def generate_hash(self, document: str) -> str:
        """Generate a unique hash for a document."""
        sha256_hash = hashlib.sha256(document.encode('utf-8')).hexdigest()

        # Use the first 32 characters of the hash to create a UUID
        return str(uuid.UUID(sha256_hash[:32]))
    
    def query(self, prompt):
        # Add user query to chat history
        self.chat_history.append({"role": "user", "content": prompt})
        
        # Ensure chat history does not exceed 10 messages (5 user + 5 AI)
        if len(self.chat_history) > 10:
            self.chat_history.pop(0)  # Remove the oldest message
        
        # Query the Qdrant client for relevant context
        results = self.client.query(
            collection_name="rag",
            query_text=prompt,
            limit=20,
        )
        result = [r.document for r in results] 
        # reranker code to have the first 5 queries 
        reranked_context = " ".join(
            str(result[i]) for i, _ in sorted(
                enumerate(self.reranker.rerank(prompt, result)),
                key=lambda x: x[1],
                reverse=True
            )[:5]
        )

        # context = "\n".join(r.document for r in results)

        # Prepare the metaprompt with chat history and context
        metaprompt = f""" 
            Use the provided context and or chat history to answer the question accurately and concisely.  
            If the answer is not explicitly stated, infer the most reasonable answer based on the available information.  

            ### Chat History:
            {self.format_chat_history()}

            ### Context:  
            {reranked_context.strip()}  

            ### Question:  
            {prompt.strip()}  

            ### Answer:
            """
        
        # Query the LLM with the metaprompt
        response = self.llm.chat.completions.create(
            model="your-model-name",
            messages=[{"role": "user", "content": metaprompt}],
            stream=True 
        )

        # Collect the AI response and add it to chat history
        full_response = ""
        for chunk in response:
            if chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
                print(chunk.choices[0].delta.content, end="", flush=True)
        
        # Add AI response to chat history
        self.chat_history.append({"role": "assistant", "content": full_response})
        
        # Ensure chat history does not exceed 10 messages after adding the AI response
        if len(self.chat_history) > 10:
            self.chat_history.pop(0)  # Remove the oldest message
        
        print(" ")

    def format_chat_history(self):
        """Format the chat history into a string for inclusion in the metaprompt."""
        formatted_history = []
        for i in range(0, len(self.chat_history), 2):
            user_message = self.chat_history[i]["content"]
            if i + 1 < len(self.chat_history):
                ai_message = self.chat_history[i + 1]["content"]
                formatted_history.append(f"User: {user_message}\nAI: {ai_message}")
            else:
                formatted_history.append(f"User: {user_message}\nAI: ")
        return "\n".join(formatted_history)

    def default(self, user_content):
        if user_content == "/bye":
            return True

        self.query(user_content)

def run_rag(vector_path):
    rag = Rag(vector_path)
    try:
        rag.cmdloop()
    except KeyboardInterrupt:
        print("")

def serve_rag(vector_path):
    rag = Rag(vector_path)
    rag.serve()

def load():
    client = qdrant_client.QdrantClient(":memory:")
    client.set_model(EMBED_MODEL)
    client.set_sparse_model(SPARSE_MODEL)
    TextCrossEncoder(model_name=RANK_MODEL)


parser = argparse.ArgumentParser(description="A script to enable Rag")
subparsers = parser.add_subparsers(dest='command')

run_parser = subparsers.add_parser('run', help='Run RAG interactively')
run_parser.add_argument("vector_path", type=str, help="Path to the vector database")
run_parser.set_defaults(func=run_rag)

serve_parser = subparsers.add_parser('serve', help='Run RAG as a service')
serve_parser.add_argument("vector_path", type=str, help="Path to the vector database")
serve_parser.set_defaults(func=serve_rag)

load_parser = subparsers.add_parser('load', help='Preload RAG Embedding Models')
load_parser.set_defaults(func=load)

try:
    args = parser.parse_args()

    if args.command:
        # Ensure that the appropriate function gets called with the right arguments
        if args.command in ['run', 'serve']:
            args.func(args.vector_path)  # pass vector_path argument to the respective function
        else:
            args.func()  # no argument for 'load'
except ValueError as e:
        eprint(e, 1)
